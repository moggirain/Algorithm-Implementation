Name: Xiaoyu Wan
Course: CSC446
Contact: xwan3@u.rochester.edu

******Files************
Wan_em_gaussian.py
readme.txt

*********instructions************************

********code explanation*********************
I implemented the Guassian Mixture Model by using the EM algorithm. 

Initial Model: In this function, three parameters lambdas, sigmas and mu were initiated. Without using the given cluster data, lambdas was initiated by 1/cluster_num and the sum probability = 1. mus was initiated randomly drawing the sample from normal distributions, as 2 k*1 matrix. 
The covariance matrix sigmas was initiated with two cases: 1) full covariance matrix-using k=cluster number identity matrix. 2) tied covariance matrix-sharing 1 identity matrix. 

train model: Both E step-Expectation and M-step Maximization of the EM algorithm were implemented for two cases. 

average likelihood: Likewise, this function is to compute the average likelihood considering the two cases above. 

Extract_weights: Extract the weights from the model after returning. 

****************Experiment*******************

In this assignment, EM algorithm is implemented for the GMM model. The dev data is used to tune and select the best log likelihood and evaluate the model.  In this case, the iteration, and number of clusters are two hyper parameters that are tuned. When experimenting the performance, The iterations(i) was tested as 300. The number of clusters are tested from 2 to 10 accordingly. 

***********Result*********************

From the range of cluster number from 2 to 10 and iteration number in 300, the experiment for the best train loss likelihood are:

best train ll -4.32844124432622
best dev: 10, 27
best dev ll -4.3307569786496

which is in iteration 10 and cluster number 299. 

**********Interpretation**************

Using the dev data as the reference to tune the hyper parameters the iterations and the number of clusters. With the number of clusters increase largely from 2 to 10, and reach the convergence rate faster. When the number of clusters are below 7, both train and development likelihood increases in a similar trend. They both started to converge around iteration 10. When the number of clusters are from 7 to 10, the likelihood of training set are converged around iteration 10 and while the development likelihood gradually decreases, which indicated the overfitting problem. As both training likelihood and development likelihood converges during the iteration os 300, I did not tune the iteration to a larger value. In sum, the result indicated that both the number of clusters and iterations affect the loss likelihood of models. 
